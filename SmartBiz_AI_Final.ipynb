{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74da53a",
   "metadata": {},
   "source": [
    "# SmartBiz AI â€“ Cleaned & Professional Notebook\n",
    "This is a polished version of the SmartBiz AI prototype notebook with cleaned outputs and improved readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-21T07:25:05.940851Z",
     "iopub.status.busy": "2025-11-21T07:25:05.940520Z",
     "iopub.status.idle": "2025-11-21T07:25:07.904436Z",
     "shell.execute_reply": "2025-11-21T07:25:07.903552Z",
     "shell.execute_reply.started": "2025-11-21T07:25:05.940825Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Installing the Google Generative AI Library**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T07:25:21.938278Z",
     "iopub.status.busy": "2025-11-21T07:25:21.937937Z",
     "iopub.status.idle": "2025-11-21T07:25:28.385293Z",
     "shell.execute_reply": "2025-11-21T07:25:28.384419Z",
     "shell.execute_reply.started": "2025-11-21T07:25:21.938251Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install google-generativeai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmartBiz AI Agents ðŸ§ ðŸ“Š\n",
    "### Enterprise Data Analytics Multi-Agent System\n",
    "\n",
    "This notebook implements a multi-agent system for enterprise data analytics \n",
    "using data cleaning, EDA, regression & non-linear models, and a Gemini-powered \n",
    "Insight Agent for business recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmartBiz AI Agents ðŸ§ ðŸ“Š\n",
    "### Enterprise Multi-Agent System for Business Data Analysis\n",
    "\n",
    "This project implements a multi-agent system for enterprise data analysis,\n",
    "including data cleaning, EDA, regression, non-linear models, and a Gemini-powered \n",
    "Insight Agent to generate business recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing the Google Generative AI package for enabling Gemini functionality**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T07:25:34.396527Z",
     "iopub.status.busy": "2025-11-21T07:25:34.396207Z",
     "iopub.status.idle": "2025-11-21T07:25:38.165900Z",
     "shell.execute_reply": "2025-11-21T07:25:38.164896Z",
     "shell.execute_reply.started": "2025-11-21T07:25:34.396497Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install google-generativeai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing all required libraries and configuring the notebook environment**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T07:25:42.136503Z",
     "iopub.status.busy": "2025-11-21T07:25:42.136172Z",
     "iopub.status.idle": "2025-11-21T07:25:47.605375Z",
     "shell.execute_reply": "2025-11-21T07:25:47.604651Z",
     "shell.execute_reply.started": "2025-11-21T07:25:42.136472Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing essential data manipulation and analysis libraries\n",
    "import pandas as pd      # Handling and analyzing structured data\n",
    "import numpy as np       # Performing numerical computations\n",
    "\n",
    "# Importing visualization library for plotting data insights\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing machine learning tools for model training and testing\n",
    "from sklearn.model_selection import train_test_split        # Splitting data into train/test sets\n",
    "from sklearn.linear_model import LinearRegression           # Building a linear regression model\n",
    "from sklearn.ensemble import RandomForestRegressor          # Building a random forest model\n",
    "from sklearn.metrics import mean_absolute_error, r2_score   # Evaluating model performance\n",
    "\n",
    "# Importing Google's Generative AI client for extended insight generation\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Importing date-time utilities and text-wrapping helper\n",
    "import datetime as dt     # Handling date and time operations\n",
    "import textwrap           # Formatting long text outputs\n",
    "\n",
    "# Configuring default plot size for cleaner visualization\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Searching through Kaggle directories to locate the exact file path of the uploaded dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T07:25:53.191507Z",
     "iopub.status.busy": "2025-11-21T07:25:53.190777Z",
     "iopub.status.idle": "2025-11-21T07:25:53.199979Z",
     "shell.execute_reply": "2025-11-21T07:25:53.199066Z",
     "shell.execute_reply.started": "2025-11-21T07:25:53.191478Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the OS module to interact with the file system\n",
    "import os\n",
    "\n",
    "# Walking through all folders and subfolders inside the /kaggle directory\n",
    "# and continuously scanning for any file that contains the words \"store\" or \"sales\"\n",
    "for root, dirs, files in os.walk(\"/kaggle\"):\n",
    "    for f in files:\n",
    "        # Converting the filename to lowercase and checking if it contains keywords\n",
    "        if \"store\" in f.lower() or \"sales\" in f.lower():\n",
    "            # Printing the complete path of the matched file to identify the correct dataset location\n",
    "            print(os.path.join(root, f))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the dataset path and configuring key column identifiers for the analysis pipeline**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T07:26:00.927359Z",
     "iopub.status.busy": "2025-11-21T07:26:00.926422Z",
     "iopub.status.idle": "2025-11-21T07:26:00.931543Z",
     "shell.execute_reply": "2025-11-21T07:26:00.930516Z",
     "shell.execute_reply.started": "2025-11-21T07:26:00.927327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting the exact file path of the dataset stored in the Kaggle environment\n",
    "DATA_PATH = \"/kaggle/input/store-sales/new_store_sales.csv\"\n",
    "\n",
    "# Specifying the target column that the model is trying to predict\n",
    "TARGET_COLUMN = \"sales\"\n",
    "\n",
    "# Identifying the date column so the pipeline can correctly parse and process time-based features\n",
    "DATE_COLUMN = \"date\"\n",
    "\n",
    "# Listing out identifier columns that should be excluded from model training (e.g., primary keys)\n",
    "ID_COLUMNS = [\"id\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Utility Tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loading the CSV file into a Pandas DataFrame.\n",
    "    Displaying the shape of the dataset after loading.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"[TOOL] Loaded data with shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def basic_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performing basic cleaning operations:\n",
    "    - Removing duplicate rows.\n",
    "    - Filling missing numeric values with the median.\n",
    "    - Trimming whitespace from text columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Removing duplicate rows\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Filling missing values in numeric columns using the median\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    # Trimming extra whitespace from text columns\n",
    "    for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    print(\"[TOOL] Basic cleaning complete.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_dates(df: pd.DataFrame, col: str):\n",
    "    \"\"\"\n",
    "    Converting the specified column into datetime format.\n",
    "    Handling invalid date formats by coercing them into NaT.\n",
    "    \"\"\"\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        print(f\"[TOOL] Parsed {col} as datetime.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def summarize_dataframe(df):\n",
    "    \"\"\"\n",
    "    Displaying summary information of the DataFrame:\n",
    "    - Showing dataframe info (columns, types, null values).\n",
    "    - Showing first few rows of the DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\nData Info:\")\n",
    "    print(df.info())\n",
    "\n",
    "    print(\"\\nHead:\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory, Logger and Base Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T07:37:39.919997Z",
     "iopub.status.busy": "2025-11-21T07:37:39.919641Z",
     "iopub.status.idle": "2025-11-21T07:37:39.927024Z",
     "shell.execute_reply": "2025-11-21T07:37:39.925994Z",
     "shell.execute_reply.started": "2025-11-21T07:37:39.919971Z"
    }
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        # Initializing an empty dictionary for storing keyâ€“value memory data\n",
    "        self.store = {}\n",
    "\n",
    "    def set(self, key, value):\n",
    "        # Saving a value in memory using the provided key\n",
    "        self.store[key] = value\n",
    "\n",
    "    def get(self, key):\n",
    "        # Retrieving a stored value using the given key\n",
    "        return self.store.get(key)\n",
    "\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self):\n",
    "        # Initializing a list for storing log messages\n",
    "        self.logs = []\n",
    "\n",
    "    def log(self, agent, message):\n",
    "        # Creating a formatted log line and storing it while also printing it\n",
    "        line = f\"[{agent}] {message}\"\n",
    "        self.logs.append(line)\n",
    "        print(line)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name, memory, logger):\n",
    "        # Initializing the agent with its name, shared memory, and logger\n",
    "        self.name = name\n",
    "        self.memory = memory\n",
    "        self.logger = logger\n",
    "\n",
    "    def log(self, msg):\n",
    "        # Logging a message with the agent's name\n",
    "        self.logger.log(self.name, msg)\n",
    "\n",
    "    def run(self, context):\n",
    "        # Raising an error because child classes are expected to implement their own run() method\n",
    "        raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining All Agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T07:51:29.272798Z",
     "iopub.status.busy": "2025-11-21T07:51:29.272441Z",
     "iopub.status.idle": "2025-11-21T07:51:29.284411Z",
     "shell.execute_reply": "2025-11-21T07:51:29.283350Z",
     "shell.execute_reply.started": "2025-11-21T07:51:29.272772Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataIngestionAgent(Agent):\n",
    "    def run(self, context):\n",
    "        # Logging that the agent is loading the data\n",
    "        self.log(\"Loading data...\")\n",
    "\n",
    "        # Loading the dataset from the file path\n",
    "        df = load_data(DATA_PATH)\n",
    "\n",
    "        # Parsing the date column into proper datetime format\n",
    "        df = parse_dates(df, DATE_COLUMN)\n",
    "\n",
    "        # Storing the raw dataframe into the context for further agents\n",
    "        context[\"raw\"] = df\n",
    "        return context\n",
    "\n",
    "\n",
    "class DataCleaningAgent(Agent):\n",
    "    def run(self, context):\n",
    "        # Logging that the agent is cleaning the data\n",
    "        self.log(\"Cleaning data...\")\n",
    "\n",
    "        # Retrieving raw data stored by the ingestion agent\n",
    "        df = context[\"raw\"]\n",
    "\n",
    "        # Performing basic data cleaning operations\n",
    "        df = basic_cleaning(df)\n",
    "\n",
    "        # Dropping rows where the target column is still missing\n",
    "        df = df.dropna(subset=[TARGET_COLUMN])\n",
    "\n",
    "        # Storing cleaned data into context\n",
    "        context[\"clean\"] = df\n",
    "        return context\n",
    "\n",
    "\n",
    "class EDAAgent(Agent):\n",
    "    def run(self, context):\n",
    "        # Logging that the agent is performing exploratory data analysis\n",
    "        self.log(\"Performing EDA...\")\n",
    "\n",
    "        # Getting cleaned data from context and summarizing it\n",
    "        df = context[\"clean\"]\n",
    "        summarize_dataframe(df)\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "class ModelingAgent(Agent):\n",
    "    def run(self, context):\n",
    "        # Logging that model training is beginning\n",
    "        self.log(\"Training models...\")\n",
    "\n",
    "        df = context[\"clean\"]\n",
    "\n",
    "        # Selecting all numeric features for modeling\n",
    "        features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        # Removing target and ID columns from the feature list\n",
    "        for c in [TARGET_COLUMN] + ID_COLUMNS:\n",
    "            if c in features:\n",
    "                features.remove(c)\n",
    "\n",
    "        # Splitting the data into features and target\n",
    "        X = df[features]\n",
    "        y = df[TARGET_COLUMN]\n",
    "\n",
    "        # Splitting into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Training a linear regression model\n",
    "        lin = LinearRegression()\n",
    "        lin.fit(X_train, y_train)\n",
    "\n",
    "        # Making predictions using the linear model\n",
    "        pred_lin = lin.predict(X_test)\n",
    "\n",
    "        # Random Forest\n",
    "        rf = RandomForestRegressor(random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        pred_rf = rf.predict(X_test)\n",
    "\n",
    "\n",
    "        # Storing model performance metrics in context\n",
    "        context[\"metrics\"] = {\n",
    "            \"linear\": mean_absolute_error(y_test, pred_lin),\n",
    "            \"rf\": mean_absolute_error(y_test, pred_rf),\n",
    "        }\n",
    "\n",
    "        # Logging the performance of each model\n",
    "        self.log(f\"Linear MAE: {context['metrics']['linear']}\")\n",
    "        self.log(f\"RF MAE: {context['metrics']['rf']}\")\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "class DecisionAgent(Agent):\n",
    "    def run(self, context):\n",
    "        # Logging that decision rules are being generated\n",
    "        self.log(\"Generating rule-based business decisions...\")\n",
    "\n",
    "        m = context[\"metrics\"]\n",
    "\n",
    "        # Comparing performance metrics and selecting the better model\n",
    "        if m[\"rf\"] < m[\"linear\"]:\n",
    "            context[\"decision\"] = \"RandomForest is performing better. Using nonlinear model.\"\n",
    "        else:\n",
    "            context[\"decision\"] = \"Linear Regression is performing similarly. Using simpler model.\"\n",
    "\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T20:57:51.723457Z",
     "iopub.status.busy": "2025-11-20T20:57:51.723171Z",
     "iopub.status.idle": "2025-11-20T20:57:51.947872Z",
     "shell.execute_reply": "2025-11-20T20:57:51.947068Z",
     "shell.execute_reply.started": "2025-11-20T20:57:51.723436Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Loading Google API key securely from Kaggle secrets\n",
    "user_secrets = UserSecretsClient()\n",
    "GOOGLE_API_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Gemini Insight Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T07:41:24.720924Z",
     "iopub.status.busy": "2025-11-21T07:41:24.720595Z",
     "iopub.status.idle": "2025-11-21T07:41:24.727472Z",
     "shell.execute_reply": "2025-11-21T07:41:24.726216Z",
     "shell.execute_reply.started": "2025-11-21T07:41:24.720900Z"
    }
   },
   "outputs": [],
   "source": [
    "class GeminiInsightAgent(Agent):\n",
    "    def run(self, context):\n",
    "        # Logging that the agent is simulating Gemini insights due to lack of internet access\n",
    "        self.log(\"Simulating Gemini insights (Kaggle has no internet)...\")\n",
    "\n",
    "        # Retrieving stored model performance metrics from context\n",
    "        metrics = context.get(\"metrics\", {})\n",
    "\n",
    "        # Retrieving the recommended decision from the DecisionAgent\n",
    "        decision = context.get(\"decision\", \"\")\n",
    "\n",
    "        # Generating a simulated AI-style business insight report\n",
    "        simulated_output = f\"\"\"\n",
    "        --- AI Business Insights (Simulated Gemini) ---\n",
    "\n",
    "        Model says:\n",
    "        {metrics}\n",
    "\n",
    "        Recommended Strategy:\n",
    "        {decision}\n",
    "\n",
    "        Additional Insights:\n",
    "        - Sales patterns are indicating that customer activity is being influenced by date, item, and store.\n",
    "        - Revenue is increasing when focusing on high-sales items.\n",
    "        - Customer engagement is varying across stores; targeted promotions are helping improve reach.\n",
    "\n",
    "        ------------------------------------------------\n",
    "        \"\"\"\n",
    "\n",
    "        # Storing the simulated insights back into context for later use\n",
    "        context[\"gemini_insights\"] = simulated_output\n",
    "\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Orchestrator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T07:42:13.870259Z",
     "iopub.status.busy": "2025-11-21T07:42:13.869917Z",
     "iopub.status.idle": "2025-11-21T07:42:13.877491Z",
     "shell.execute_reply": "2025-11-21T07:42:13.876647Z",
     "shell.execute_reply.started": "2025-11-21T07:42:13.870234Z"
    }
   },
   "outputs": [],
   "source": [
    "class Orchestrator:\n",
    "    def __init__(self):\n",
    "        # Initializing shared memory and logger for all agents\n",
    "        self.memory = Memory()\n",
    "        self.logger = Logger()\n",
    "\n",
    "        # Initializing all agents with shared memory and logging system\n",
    "        self.ingest = DataIngestionAgent(\"Ingest\", self.memory, self.logger)\n",
    "        self.clean = DataCleaningAgent(\"Clean\", self.memory, self.logger)\n",
    "        self.eda = EDAAgent(\"EDA\", self.memory, self.logger)\n",
    "        self.model = ModelingAgent(\"Model\", self.memory, self.logger)\n",
    "        self.decision = DecisionAgent(\"Decision\", self.memory, self.logger)\n",
    "        self.gemini = GeminiInsightAgent(\"Gemini\", self.memory, self.logger)\n",
    "\n",
    "    def run(self):\n",
    "        # Creating an empty context dictionary for passing data between agents\n",
    "        context = {}\n",
    "\n",
    "        # Running the data ingestion agent\n",
    "        context = self.ingest.run(context)\n",
    "\n",
    "        # Running the data cleaning agent\n",
    "        context = self.clean.run(context)\n",
    "\n",
    "        # Running the EDA agent to analyze the cleaned data\n",
    "        context = self.eda.run(context)\n",
    "\n",
    "        # Running the modeling agent to train and evaluate models\n",
    "        context = self.model.run(context)\n",
    "\n",
    "        # Running the decision agent to create rule-based model recommendations\n",
    "        context = self.decision.run(context)\n",
    "\n",
    "        # Running the simulated Gemini insights agent to generate business insights\n",
    "        context = self.gemini.run(context)\n",
    "\n",
    "        # Returning the final aggregated context containing all outputs\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Runing the FULL PIPELINE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T07:51:37.654923Z",
     "iopub.status.busy": "2025-11-21T07:51:37.654573Z",
     "iopub.status.idle": "2025-11-21T07:51:37.808834Z",
     "shell.execute_reply": "2025-11-21T07:51:37.808046Z",
     "shell.execute_reply.started": "2025-11-21T07:51:37.654897Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating an instance of the Orchestrator, which is initializing all agents\n",
    "orch = Orchestrator()\n",
    "\n",
    "# Running the full pipeline and collecting all outputs in the context dictionary\n",
    "context = orch.run()\n",
    "\n",
    "# Displaying the final business decision generated by the DecisionAgent\n",
    "print(\"\\n=== FINAL BUSINESS DECISION ===\")\n",
    "print(context.get(\"decision\"))\n",
    "\n",
    "# Displaying the simulated Gemini insights generated by the GeminiInsightAgent\n",
    "print(\"\\n=== GEMINI INSIGHTS ===\")\n",
    "print(context.get(\"gemini_insights\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8793061,
     "sourceId": 13809037,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
