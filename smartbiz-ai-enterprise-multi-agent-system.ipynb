{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13809037,"sourceType":"datasetVersion","datasetId":8793061}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T07:25:05.940520Z","iopub.execute_input":"2025-11-21T07:25:05.940851Z","iopub.status.idle":"2025-11-21T07:25:07.904436Z","shell.execute_reply.started":"2025-11-21T07:25:05.940825Z","shell.execute_reply":"2025-11-21T07:25:07.903552Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"** Installing the Google Generative AI Library**\n","metadata":{}},{"cell_type":"code","source":"!pip install google-generativeai\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T07:25:21.937937Z","iopub.execute_input":"2025-11-21T07:25:21.938278Z","iopub.status.idle":"2025-11-21T07:25:28.385293Z","shell.execute_reply.started":"2025-11-21T07:25:21.938251Z","shell.execute_reply":"2025-11-21T07:25:28.384419Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SmartBiz AI Agents ðŸ§ ðŸ“Š\n### Enterprise Data Analytics Multi-Agent System\n\nThis notebook implements a multi-agent system for enterprise data analytics \nusing data cleaning, EDA, regression & non-linear models, and a Gemini-powered \nInsight Agent for business recommendations.\n","metadata":{}},{"cell_type":"markdown","source":"# SmartBiz AI Agents ðŸ§ ðŸ“Š\n### Enterprise Multi-Agent System for Business Data Analysis\n\nThis project implements a multi-agent system for enterprise data analysis,\nincluding data cleaning, EDA, regression, non-linear models, and a Gemini-powered \nInsight Agent to generate business recommendations.\n","metadata":{}},{"cell_type":"markdown","source":"**Installing the Google Generative AI package for enabling Gemini functionality**\n","metadata":{}},{"cell_type":"code","source":"!pip install google-generativeai\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T07:25:34.396207Z","iopub.execute_input":"2025-11-21T07:25:34.396527Z","iopub.status.idle":"2025-11-21T07:25:38.165900Z","shell.execute_reply.started":"2025-11-21T07:25:34.396497Z","shell.execute_reply":"2025-11-21T07:25:38.164896Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Importing all required libraries and configuring the notebook environment**\n","metadata":{}},{"cell_type":"code","source":"# Importing essential data manipulation and analysis libraries\nimport pandas as pd      # Handling and analyzing structured data\nimport numpy as np       # Performing numerical computations\n\n# Importing visualization library for plotting data insights\nimport matplotlib.pyplot as plt\n\n# Importing machine learning tools for model training and testing\nfrom sklearn.model_selection import train_test_split        # Splitting data into train/test sets\nfrom sklearn.linear_model import LinearRegression           # Building a linear regression model\nfrom sklearn.ensemble import RandomForestRegressor          # Building a random forest model\nfrom sklearn.metrics import mean_absolute_error, r2_score   # Evaluating model performance\n\n# Importing Google's Generative AI client for extended insight generation\nimport google.generativeai as genai\n\n# Importing date-time utilities and text-wrapping helper\nimport datetime as dt     # Handling date and time operations\nimport textwrap           # Formatting long text outputs\n\n# Configuring default plot size for cleaner visualization\nplt.rcParams[\"figure.figsize\"] = (8, 4)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T07:25:42.136172Z","iopub.execute_input":"2025-11-21T07:25:42.136503Z","iopub.status.idle":"2025-11-21T07:25:47.605375Z","shell.execute_reply.started":"2025-11-21T07:25:42.136472Z","shell.execute_reply":"2025-11-21T07:25:47.604651Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Searching through Kaggle directories to locate the exact file path of the uploaded dataset**\n","metadata":{}},{"cell_type":"code","source":"# Importing the OS module to interact with the file system\nimport os\n\n# Walking through all folders and subfolders inside the /kaggle directory\n# and continuously scanning for any file that contains the words \"store\" or \"sales\"\nfor root, dirs, files in os.walk(\"/kaggle\"):\n    for f in files:\n        # Converting the filename to lowercase and checking if it contains keywords\n        if \"store\" in f.lower() or \"sales\" in f.lower():\n            # Printing the complete path of the matched file to identify the correct dataset location\n            print(os.path.join(root, f))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T07:25:53.190777Z","iopub.execute_input":"2025-11-21T07:25:53.191507Z","iopub.status.idle":"2025-11-21T07:25:53.199979Z","shell.execute_reply.started":"2025-11-21T07:25:53.191478Z","shell.execute_reply":"2025-11-21T07:25:53.199066Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Defining the dataset path and configuring key column identifiers for the analysis pipeline**\n","metadata":{}},{"cell_type":"code","source":"# Setting the exact file path of the dataset stored in the Kaggle environment\nDATA_PATH = \"/kaggle/input/store-sales/new_store_sales.csv\"\n\n# Specifying the target column that the model is trying to predict\nTARGET_COLUMN = \"sales\"\n\n# Identifying the date column so the pipeline can correctly parse and process time-based features\nDATE_COLUMN = \"date\"\n\n# Listing out identifier columns that should be excluded from model training (e.g., primary keys)\nID_COLUMNS = [\"id\"]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T07:26:00.926422Z","iopub.execute_input":"2025-11-21T07:26:00.927359Z","iopub.status.idle":"2025-11-21T07:26:00.931543Z","shell.execute_reply.started":"2025-11-21T07:26:00.927327Z","shell.execute_reply":"2025-11-21T07:26:00.930516Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Adding Utility Tools**","metadata":{}},{"cell_type":"code","source":"def load_data(path: str) -> pd.DataFrame:\n    \"\"\"\n    Loading the CSV file into a Pandas DataFrame.\n    Displaying the shape of the dataset after loading.\n    \"\"\"\n    df = pd.read_csv(path)\n    print(f\"[TOOL] Loaded data with shape: {df.shape}\")\n    return df\n\n\ndef basic_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Performing basic cleaning operations:\n    - Removing duplicate rows.\n    - Filling missing numeric values with the median.\n    - Trimming whitespace from text columns.\n    \"\"\"\n    df = df.copy()\n\n    # Removing duplicate rows\n    df = df.drop_duplicates()\n\n    # Filling missing values in numeric columns using the median\n    for col in df.select_dtypes(include=[np.number]).columns:\n        df[col] = df[col].fillna(df[col].median())\n\n    # Trimming extra whitespace from text columns\n    for col in df.select_dtypes(include=[\"object\"]).columns:\n        df[col] = df[col].astype(str).str.strip()\n\n    print(\"[TOOL] Basic cleaning complete.\")\n    return df\n\n\ndef parse_dates(df: pd.DataFrame, col: str):\n    \"\"\"\n    Converting the specified column into datetime format.\n    Handling invalid date formats by coercing them into NaT.\n    \"\"\"\n    if col in df.columns:\n        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n        print(f\"[TOOL] Parsed {col} as datetime.\")\n    return df\n\n\ndef summarize_dataframe(df):\n    \"\"\"\n    Displaying summary information of the DataFrame:\n    - Showing dataframe info (columns, types, null values).\n    - Showing first few rows of the DataFrame.\n    \"\"\"\n    print(\"\\nData Info:\")\n    print(df.info())\n\n    print(\"\\nHead:\")\n    print(df.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Memory, Logger and Base Agent**","metadata":{}},{"cell_type":"code","source":"class Memory:\n    def __init__(self):\n        # Initializing an empty dictionary for storing keyâ€“value memory data\n        self.store = {}\n\n    def set(self, key, value):\n        # Saving a value in memory using the provided key\n        self.store[key] = value\n\n    def get(self, key):\n        # Retrieving a stored value using the given key\n        return self.store.get(key)\n\n\nclass Logger:\n    def __init__(self):\n        # Initializing a list for storing log messages\n        self.logs = []\n\n    def log(self, agent, message):\n        # Creating a formatted log line and storing it while also printing it\n        line = f\"[{agent}] {message}\"\n        self.logs.append(line)\n        print(line)\n\n\nclass Agent:\n    def __init__(self, name, memory, logger):\n        # Initializing the agent with its name, shared memory, and logger\n        self.name = name\n        self.memory = memory\n        self.logger = logger\n\n    def log(self, msg):\n        # Logging a message with the agent's name\n        self.logger.log(self.name, msg)\n\n    def run(self, context):\n        # Raising an error because child classes are expected to implement their own run() method\n        raise NotImplementedError\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T07:37:39.919641Z","iopub.execute_input":"2025-11-21T07:37:39.919997Z","iopub.status.idle":"2025-11-21T07:37:39.927024Z","shell.execute_reply.started":"2025-11-21T07:37:39.919971Z","shell.execute_reply":"2025-11-21T07:37:39.925994Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Defining All Agents**","metadata":{}},{"cell_type":"code","source":"class DataIngestionAgent(Agent):\n    def run(self, context):\n        # Logging that the agent is loading the data\n        self.log(\"Loading data...\")\n\n        # Loading the dataset from the file path\n        df = load_data(DATA_PATH)\n\n        # Parsing the date column into proper datetime format\n        df = parse_dates(df, DATE_COLUMN)\n\n        # Storing the raw dataframe into the context for further agents\n        context[\"raw\"] = df\n        return context\n\n\nclass DataCleaningAgent(Agent):\n    def run(self, context):\n        # Logging that the agent is cleaning the data\n        self.log(\"Cleaning data...\")\n\n        # Retrieving raw data stored by the ingestion agent\n        df = context[\"raw\"]\n\n        # Performing basic data cleaning operations\n        df = basic_cleaning(df)\n\n        # Dropping rows where the target column is still missing\n        df = df.dropna(subset=[TARGET_COLUMN])\n\n        # Storing cleaned data into context\n        context[\"clean\"] = df\n        return context\n\n\nclass EDAAgent(Agent):\n    def run(self, context):\n        # Logging that the agent is performing exploratory data analysis\n        self.log(\"Performing EDA...\")\n\n        # Getting cleaned data from context and summarizing it\n        df = context[\"clean\"]\n        summarize_dataframe(df)\n\n        return context\n\n\nclass ModelingAgent(Agent):\n    def run(self, context):\n        # Logging that model training is beginning\n        self.log(\"Training models...\")\n\n        df = context[\"clean\"]\n\n        # Selecting all numeric features for modeling\n        features = df.select_dtypes(include=[np.number]).columns.tolist()\n\n        # Removing target and ID columns from the feature list\n        for c in [TARGET_COLUMN] + ID_COLUMNS:\n            if c in features:\n                features.remove(c)\n\n        # Splitting the data into features and target\n        X = df[features]\n        y = df[TARGET_COLUMN]\n\n        # Splitting into training and testing sets\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2, random_state=42\n        )\n\n        # Training a linear regression model\n        lin = LinearRegression()\n        lin.fit(X_train, y_train)\n\n        # Making predictions using the linear model\n        pred_lin = lin.predict(X_test)\n\n        # Random Forest\n        rf = RandomForestRegressor(random_state=42)\n        rf.fit(X_train, y_train)\n        pred_rf = rf.predict(X_test)\n\n\n        # Storing model performance metrics in context\n        context[\"metrics\"] = {\n            \"linear\": mean_absolute_error(y_test, pred_lin),\n            \"rf\": mean_absolute_error(y_test, pred_rf),\n        }\n\n        # Logging the performance of each model\n        self.log(f\"Linear MAE: {context['metrics']['linear']}\")\n        self.log(f\"RF MAE: {context['metrics']['rf']}\")\n\n        return context\n\n\nclass DecisionAgent(Agent):\n    def run(self, context):\n        # Logging that decision rules are being generated\n        self.log(\"Generating rule-based business decisions...\")\n\n        m = context[\"metrics\"]\n\n        # Comparing performance metrics and selecting the better model\n        if m[\"rf\"] < m[\"linear\"]:\n            context[\"decision\"] = \"RandomForest is performing better. Using nonlinear model.\"\n        else:\n            context[\"decision\"] = \"Linear Regression is performing similarly. Using simpler model.\"\n\n        return context\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T07:51:29.272441Z","iopub.execute_input":"2025-11-21T07:51:29.272798Z","iopub.status.idle":"2025-11-21T07:51:29.284411Z","shell.execute_reply.started":"2025-11-21T07:51:29.272772Z","shell.execute_reply":"2025-11-21T07:51:29.283350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport google.generativeai as genai\n\n# Loading Google API key securely from Kaggle secrets\nuser_secrets = UserSecretsClient()\nGOOGLE_API_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T20:57:51.723171Z","iopub.execute_input":"2025-11-20T20:57:51.723457Z","iopub.status.idle":"2025-11-20T20:57:51.947872Z","shell.execute_reply.started":"2025-11-20T20:57:51.723436Z","shell.execute_reply":"2025-11-20T20:57:51.947068Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Adding Gemini Insight Agent**","metadata":{}},{"cell_type":"code","source":"class GeminiInsightAgent(Agent):\n    def run(self, context):\n        # Logging that the agent is simulating Gemini insights due to lack of internet access\n        self.log(\"Simulating Gemini insights (Kaggle has no internet)...\")\n\n        # Retrieving stored model performance metrics from context\n        metrics = context.get(\"metrics\", {})\n\n        # Retrieving the recommended decision from the DecisionAgent\n        decision = context.get(\"decision\", \"\")\n\n        # Generating a simulated AI-style business insight report\n        simulated_output = f\"\"\"\n        --- AI Business Insights (Simulated Gemini) ---\n\n        Model says:\n        {metrics}\n\n        Recommended Strategy:\n        {decision}\n\n        Additional Insights:\n        - Sales patterns are indicating that customer activity is being influenced by date, item, and store.\n        - Revenue is increasing when focusing on high-sales items.\n        - Customer engagement is varying across stores; targeted promotions are helping improve reach.\n\n        ------------------------------------------------\n        \"\"\"\n\n        # Storing the simulated insights back into context for later use\n        context[\"gemini_insights\"] = simulated_output\n\n        return context\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T07:41:24.720595Z","iopub.execute_input":"2025-11-21T07:41:24.720924Z","iopub.status.idle":"2025-11-21T07:41:24.727472Z","shell.execute_reply.started":"2025-11-21T07:41:24.720900Z","shell.execute_reply":"2025-11-21T07:41:24.726216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Orchestrator**","metadata":{}},{"cell_type":"code","source":"class Orchestrator:\n    def __init__(self):\n        # Initializing shared memory and logger for all agents\n        self.memory = Memory()\n        self.logger = Logger()\n\n        # Initializing all agents with shared memory and logging system\n        self.ingest = DataIngestionAgent(\"Ingest\", self.memory, self.logger)\n        self.clean = DataCleaningAgent(\"Clean\", self.memory, self.logger)\n        self.eda = EDAAgent(\"EDA\", self.memory, self.logger)\n        self.model = ModelingAgent(\"Model\", self.memory, self.logger)\n        self.decision = DecisionAgent(\"Decision\", self.memory, self.logger)\n        self.gemini = GeminiInsightAgent(\"Gemini\", self.memory, self.logger)\n\n    def run(self):\n        # Creating an empty context dictionary for passing data between agents\n        context = {}\n\n        # Running the data ingestion agent\n        context = self.ingest.run(context)\n\n        # Running the data cleaning agent\n        context = self.clean.run(context)\n\n        # Running the EDA agent to analyze the cleaned data\n        context = self.eda.run(context)\n\n        # Running the modeling agent to train and evaluate models\n        context = self.model.run(context)\n\n        # Running the decision agent to create rule-based model recommendations\n        context = self.decision.run(context)\n\n        # Running the simulated Gemini insights agent to generate business insights\n        context = self.gemini.run(context)\n\n        # Returning the final aggregated context containing all outputs\n        return context\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T07:42:13.869917Z","iopub.execute_input":"2025-11-21T07:42:13.870259Z","iopub.status.idle":"2025-11-21T07:42:13.877491Z","shell.execute_reply.started":"2025-11-21T07:42:13.870234Z","shell.execute_reply":"2025-11-21T07:42:13.876647Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Runing the FULL PIPELINE**","metadata":{}},{"cell_type":"code","source":"# Creating an instance of the Orchestrator, which is initializing all agents\norch = Orchestrator()\n\n# Running the full pipeline and collecting all outputs in the context dictionary\ncontext = orch.run()\n\n# Displaying the final business decision generated by the DecisionAgent\nprint(\"\\n=== FINAL BUSINESS DECISION ===\")\nprint(context.get(\"decision\"))\n\n# Displaying the simulated Gemini insights generated by the GeminiInsightAgent\nprint(\"\\n=== GEMINI INSIGHTS ===\")\nprint(context.get(\"gemini_insights\"))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T07:51:37.654573Z","iopub.execute_input":"2025-11-21T07:51:37.654923Z","iopub.status.idle":"2025-11-21T07:51:37.808834Z","shell.execute_reply.started":"2025-11-21T07:51:37.654897Z","shell.execute_reply":"2025-11-21T07:51:37.808046Z"}},"outputs":[],"execution_count":null}]}